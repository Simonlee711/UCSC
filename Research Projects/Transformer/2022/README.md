# Transformer Attention Models on NeurIPS 2021 Problem

Single-cell transcriptome sequencing (scRNA-seq) research has recently become one of the newest and widely used methods for biological research due to its ability to gain a more comprehensive understanding of the biology of a cell.  These recent advances in single-cell genomics technologies have enabled investigation of the gene regulation programs of multi-cellular organisms at unprecedented resolution and scale. Development of single-cell multi-modal omics tools is therefore another major step toward understanding the inner workings of biological systems. However one of the primary limitations that is currently being researched is how to best construct these multi-modal tools to be deemed helpful in this field. For this reason our laboratory has experimented using the \emph{t5-small}, an attention based Natural Language Processing model developed by \emph{Google}. By playing around with its parameters and its architecture, we have developed a novel approach to "translating" the code of life for multi-modal prediction. Being able to get an understanding of this single-cell measurement data can help prompt a leap in scientific discovery in trying to comprehend the causes of complex disease and biological diversity.
