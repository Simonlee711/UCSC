{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1726ddd1",
   "metadata": {},
   "source": [
    "# Transformer model for predicting modalities in scRNA-seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a670f",
   "metadata": {},
   "source": [
    "**Authors**<br>Vedu Mallela: GiwoTech, vedu.mallela@gmail.com<br>Simon Lee: UC Santa Cruz, siaulee@ucsc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5ff58",
   "metadata": {},
   "source": [
    "# Goal of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c33c9",
   "metadata": {},
   "source": [
    "**TODO: explain algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc46e87",
   "metadata": {},
   "source": [
    "# Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf91ff",
   "metadata": {},
   "source": [
    "Import all files and modules for this competition<br>\n",
    "*below will provide documentation of the following libraries*<br>\n",
    "<br>\n",
    "**scanpy** (**s**ingle **c**ell **an**alysis in **Py**thon) - https://scanpy.readthedocs.io/en/stable/ <br>\n",
    "**anndata** (**ann**otated **data**) - https://anndata.readthedocs.io/en/latest/ <br>\n",
    "**matplotlib** - https://matplotlib.org/ <br>\n",
    "**numpy** - https://numpy.org/doc/stable/ <br>\n",
    "**pandas** - https://pandas.pydata.org/ <br>\n",
    "**logging** - https://docs.python.org/3/howto/logging.html <br>\n",
    "**sklearn** - https://scikit-learn.org/stable/ <br>\n",
    "<br>\n",
    "*code begins here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a0c65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59dd606",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91dad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/tmp/public/multiome/'\n",
    "outpath ='../out/'\n",
    "adata_gex = ad.read_h5ad(path + \"multiome_gex_processed_training.h5ad\")\n",
    "adata_atac = ad.read_h5ad(path + \"multiome_atac_processed_training.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4664e",
   "metadata": {},
   "source": [
    "After successfully loading in the data, we can try to begin plotting the batch for the **Assay for Transposase-Accessible Chromatin using sequencing** (ATAC-seq) and **Gene Expression** (GEX) data on the umap interface. These umap projections will be saved to GEX.pdf and ATAC.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83dbe4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.tl.pca(adata_gex)\n",
    "#sc.pl.umap(adata_gex, color=['batch'],save='_GEX', title='GEX umap Display')\n",
    "#sc.tl.pca(adata_atac)\n",
    "#sc.pl.umap(adata_atac, color=['batch'], layer='counts', save='_ATAC', title='ATAC umap Display')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0acfe1",
   "metadata": {},
   "source": [
    "Next we are going to check out all the indivdual cell types occuring in both the ATAC and GEX data. This way we can see all the types of cells from this dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4504456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.tl.pca(adata_gex)\n",
    "#sc.pl.umap(adata_gex, color='cell_type',save='_GEX_ct', title='GEX Cell Type umap')\n",
    "#sc.tl.pca(adata_atac)\n",
    "#sc.pl.umap(adata_atac, color='cell_type',save='_ATAC_ct', title='ATAC Cell Type umap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d057a33",
   "metadata": {},
   "source": [
    "# Filter out our Data <1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84155657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the data\n",
    "# Convert anndata objects to dataframes and filter.  \n",
    "# Genes that show up in < 1% cells are dropped.\n",
    "# Atac seq data that shows up in < 1% cells are dropped \n",
    "\n",
    "gex_df = adata_gex.to_df()\n",
    "atac_df = adata_atac.to_df()\n",
    "\n",
    "gex_df_col = np.array(gex_df.columns.values)\n",
    "atac_df_col = np.array(atac_df.columns.values)\n",
    "\n",
    "# filter out the data\n",
    "# Convert anndata objects to dataframes and filter.  \n",
    "# Genes that show up in < 1% cells are dropped.\n",
    "# Atac seq data that shows up in < 1% cells are dropped \n",
    "\n",
    "gex_df = adata_gex.to_df()\n",
    "atac_df = adata_atac.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e457016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22463\n",
      "22463\n"
     ]
    }
   ],
   "source": [
    "gex_df_row = list(gex_df.index.values)\n",
    "atac_df_row = list(atac_df.index.values)\n",
    "\n",
    "for row in gex_df_row: # delete the differences rows between atac and gex\n",
    "    if row not in atac_df_row:\n",
    "        gex_df_row.remove(row)\n",
    "        \n",
    "# initialize dictionary to store the results\n",
    "gex_dictionary = {key: None for key in gex_df_row}\n",
    "\n",
    "print(len(gex_dictionary))\n",
    "print(len(gex_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5516e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data set\n",
      "GEX data: Total cells=22463, Number features=12160\n",
      "ATAC data: Total cells=22463, Number Features=53020\n"
     ]
    }
   ],
   "source": [
    "mask = gex_df>0\n",
    "total_cells = gex_df.shape[0]\n",
    "maskdf = mask.sum(axis=0)/total_cells*100 <=1\n",
    "\n",
    "gex_feature_drop  = list(maskdf.loc[maskdf==True].index.values)\n",
    "\n",
    "mask = atac_df>0\n",
    "maskdf = mask.sum(axis=0)/total_cells <=0.01\n",
    "\n",
    "atac_feature_drop  = list(maskdf.loc[maskdf==True].index.values)\n",
    "\n",
    "gex_ = gex_df.drop(columns=gex_feature_drop)\n",
    "atac_ = atac_df.drop(columns=atac_feature_drop)\n",
    "\n",
    "print('Filtered data set')\n",
    "print('GEX data: Total cells=' + str(gex_.shape[0]) + ', Number features=' + str(gex_.shape[1]))\n",
    "print('ATAC data: Total cells=' + str(atac_.shape[0]) + ', Number Features=' + str(atac_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf9b3c",
   "metadata": {},
   "source": [
    "# Filtered out Gene Expression and ATAC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2c19b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AL627309.5</th>\n",
       "      <th>LINC01409</th>\n",
       "      <th>LINC01128</th>\n",
       "      <th>NOC2L</th>\n",
       "      <th>ISG15</th>\n",
       "      <th>C1orf159</th>\n",
       "      <th>SDF4</th>\n",
       "      <th>B3GALT6</th>\n",
       "      <th>UBE2J2</th>\n",
       "      <th>ACAP3</th>\n",
       "      <th>...</th>\n",
       "      <th>MT-ATP8</th>\n",
       "      <th>MT-ATP6</th>\n",
       "      <th>MT-CO3</th>\n",
       "      <th>MT-ND3</th>\n",
       "      <th>MT-ND4L</th>\n",
       "      <th>MT-ND4</th>\n",
       "      <th>MT-ND5</th>\n",
       "      <th>MT-ND6</th>\n",
       "      <th>MT-CYB</th>\n",
       "      <th>AL592183.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TAGTTGTCACCCTCAC-1-s1d1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.410295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.410295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.410295</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTATGGCCATAACGGG-1-s1d1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.194758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.194758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.168547</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCGCACACAGGTTAAA-1-s1d1</th>\n",
       "      <td>0.410619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410619</td>\n",
       "      <td>0.821238</td>\n",
       "      <td>3.284951</td>\n",
       "      <td>0.410619</td>\n",
       "      <td>0.410619</td>\n",
       "      <td>0.821238</td>\n",
       "      <td>1.231857</td>\n",
       "      <td>0.410619</td>\n",
       "      <td>3.284951</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCATTTGGTAATGGAA-1-s1d1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.879966</td>\n",
       "      <td>11.519863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.759931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACCACATAGGTGTCCA-1-s1d1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.743880</td>\n",
       "      <td>13.103581</td>\n",
       "      <td>1.871940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.743880</td>\n",
       "      <td>1.871940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.359701</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAGTAAGCAACTAGGG-8-s3d6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.915288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TGGTCCTTCGGCTAGC-8-s3d6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGCTTGCGTTGTTGGA-8-s3d6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.427911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACCCTCCCAGCCAGTT-8-s3d6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGTGAACCATCCCGCT-8-s3d6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.338799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.338799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22463 rows × 12160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         AL627309.5  LINC01409  LINC01128  NOC2L     ISG15  \\\n",
       "TAGTTGTCACCCTCAC-1-s1d1    0.000000        0.0        0.0    0.0  0.000000   \n",
       "CTATGGCCATAACGGG-1-s1d1    0.000000        0.0        0.0    0.0  0.000000   \n",
       "CCGCACACAGGTTAAA-1-s1d1    0.410619        0.0        0.0    0.0  0.000000   \n",
       "TCATTTGGTAATGGAA-1-s1d1    0.000000        0.0        0.0    0.0  0.000000   \n",
       "ACCACATAGGTGTCCA-1-s1d1    0.000000        0.0        0.0    0.0  0.000000   \n",
       "...                             ...        ...        ...    ...       ...   \n",
       "TAGTAAGCAACTAGGG-8-s3d6    0.000000        0.0        0.0    0.0  1.915288   \n",
       "TGGTCCTTCGGCTAGC-8-s3d6    0.000000        0.0        0.0    0.0  0.000000   \n",
       "CGCTTGCGTTGTTGGA-8-s3d6    0.000000        0.0        0.0    0.0  0.000000   \n",
       "ACCCTCCCAGCCAGTT-8-s3d6    0.000000        0.0        0.0    0.0  0.000000   \n",
       "AGTGAACCATCCCGCT-8-s3d6    0.000000        0.0        0.0    0.0  0.000000   \n",
       "\n",
       "                         C1orf159  SDF4  B3GALT6    UBE2J2  ACAP3  ...  \\\n",
       "TAGTTGTCACCCTCAC-1-s1d1       0.0   0.0      0.0  0.000000    0.0  ...   \n",
       "CTATGGCCATAACGGG-1-s1d1       0.0   0.0      0.0  2.194758    0.0  ...   \n",
       "CCGCACACAGGTTAAA-1-s1d1       0.0   0.0      0.0  0.000000    0.0  ...   \n",
       "TCATTTGGTAATGGAA-1-s1d1       0.0   0.0      0.0  0.000000    0.0  ...   \n",
       "ACCACATAGGTGTCCA-1-s1d1       0.0   0.0      0.0  0.000000    0.0  ...   \n",
       "...                           ...   ...      ...       ...    ...  ...   \n",
       "TAGTAAGCAACTAGGG-8-s3d6       0.0   0.0      0.0  0.000000    0.0  ...   \n",
       "TGGTCCTTCGGCTAGC-8-s3d6       0.0   0.0      0.0  0.000000    0.0  ...   \n",
       "CGCTTGCGTTGTTGGA-8-s3d6       0.0   0.0      0.0  3.427911    0.0  ...   \n",
       "ACCCTCCCAGCCAGTT-8-s3d6       0.0   0.0      0.0  0.000000    0.0  ...   \n",
       "AGTGAACCATCCCGCT-8-s3d6       0.0   0.0      0.0  1.338799    0.0  ...   \n",
       "\n",
       "                          MT-ATP8   MT-ATP6     MT-CO3    MT-ND3   MT-ND4L  \\\n",
       "TAGTTGTCACCCTCAC-1-s1d1  0.000000  0.000000   0.000000  4.410295  0.000000   \n",
       "CTATGGCCATAACGGG-1-s1d1  0.000000  0.000000   2.194758  0.000000  0.000000   \n",
       "CCGCACACAGGTTAAA-1-s1d1  0.410619  0.821238   3.284951  0.410619  0.410619   \n",
       "TCATTTGGTAATGGAA-1-s1d1  0.000000  2.879966  11.519863  0.000000  0.000000   \n",
       "ACCACATAGGTGTCCA-1-s1d1  0.000000  3.743880  13.103581  1.871940  0.000000   \n",
       "...                           ...       ...        ...       ...       ...   \n",
       "TAGTAAGCAACTAGGG-8-s3d6  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "TGGTCCTTCGGCTAGC-8-s3d6  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "CGCTTGCGTTGTTGGA-8-s3d6  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "ACCCTCCCAGCCAGTT-8-s3d6  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "AGTGAACCATCCCGCT-8-s3d6  0.000000  1.338799   0.000000  0.000000  0.000000   \n",
       "\n",
       "                           MT-ND4    MT-ND5    MT-ND6     MT-CYB  AL592183.1  \n",
       "TAGTTGTCACCCTCAC-1-s1d1  4.410295  0.000000  0.000000   4.410295         0.0  \n",
       "CTATGGCCATAACGGG-1-s1d1  0.000000  0.000000  0.000000  13.168547         0.0  \n",
       "CCGCACACAGGTTAAA-1-s1d1  0.821238  1.231857  0.410619   3.284951         0.0  \n",
       "TCATTTGGTAATGGAA-1-s1d1  0.000000  5.759931  0.000000   0.000000         0.0  \n",
       "ACCACATAGGTGTCCA-1-s1d1  3.743880  1.871940  0.000000   9.359701         0.0  \n",
       "...                           ...       ...       ...        ...         ...  \n",
       "TAGTAAGCAACTAGGG-8-s3d6  0.000000  0.000000  0.000000   0.000000         0.0  \n",
       "TGGTCCTTCGGCTAGC-8-s3d6  0.000000  0.000000  0.000000   0.000000         0.0  \n",
       "CGCTTGCGTTGTTGGA-8-s3d6  0.000000  0.000000  0.000000   0.000000         0.0  \n",
       "ACCCTCCCAGCCAGTT-8-s3d6  0.000000  0.000000  0.000000   0.000000         0.0  \n",
       "AGTGAACCATCCCGCT-8-s3d6  0.000000  0.000000  0.000000   0.000000         0.0  \n",
       "\n",
       "[22463 rows x 12160 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gex_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da424d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr1-181117-181803</th>\n",
       "      <th>chr1-629497-630394</th>\n",
       "      <th>chr1-633515-634474</th>\n",
       "      <th>chr1-778276-779191</th>\n",
       "      <th>chr1-816868-817761</th>\n",
       "      <th>chr1-827067-827948</th>\n",
       "      <th>chr1-842497-843414</th>\n",
       "      <th>chr1-869472-870377</th>\n",
       "      <th>chr1-904343-905196</th>\n",
       "      <th>chr1-906441-907357</th>\n",
       "      <th>...</th>\n",
       "      <th>GL000205.2-88673-89483</th>\n",
       "      <th>GL000205.2-140307-141166</th>\n",
       "      <th>GL000195.1-30407-31261</th>\n",
       "      <th>GL000195.1-32211-33062</th>\n",
       "      <th>GL000219.1-39933-40839</th>\n",
       "      <th>GL000219.1-42172-43054</th>\n",
       "      <th>GL000219.1-44703-45584</th>\n",
       "      <th>GL000219.1-45726-46450</th>\n",
       "      <th>GL000219.1-99257-100160</th>\n",
       "      <th>KI270713.1-21434-22336</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TAGTTGTCACCCTCAC-1-s1d1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTATGGCCATAACGGG-1-s1d1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCGCACACAGGTTAAA-1-s1d1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCATTTGGTAATGGAA-1-s1d1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACCACATAGGTGTCCA-1-s1d1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAGTAAGCAACTAGGG-8-s3d6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TGGTCCTTCGGCTAGC-8-s3d6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGCTTGCGTTGTTGGA-8-s3d6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACCCTCCCAGCCAGTT-8-s3d6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGTGAACCATCCCGCT-8-s3d6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22463 rows × 53020 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         chr1-181117-181803  chr1-629497-630394  \\\n",
       "TAGTTGTCACCCTCAC-1-s1d1                 0.0                 0.0   \n",
       "CTATGGCCATAACGGG-1-s1d1                 0.0                 1.0   \n",
       "CCGCACACAGGTTAAA-1-s1d1                 0.0                 0.0   \n",
       "TCATTTGGTAATGGAA-1-s1d1                 0.0                 0.0   \n",
       "ACCACATAGGTGTCCA-1-s1d1                 0.0                 0.0   \n",
       "...                                     ...                 ...   \n",
       "TAGTAAGCAACTAGGG-8-s3d6                 0.0                 0.0   \n",
       "TGGTCCTTCGGCTAGC-8-s3d6                 0.0                 1.0   \n",
       "CGCTTGCGTTGTTGGA-8-s3d6                 0.0                 0.0   \n",
       "ACCCTCCCAGCCAGTT-8-s3d6                 0.0                 0.0   \n",
       "AGTGAACCATCCCGCT-8-s3d6                 0.0                 0.0   \n",
       "\n",
       "                         chr1-633515-634474  chr1-778276-779191  \\\n",
       "TAGTTGTCACCCTCAC-1-s1d1                 1.0                 0.0   \n",
       "CTATGGCCATAACGGG-1-s1d1                 1.0                 0.0   \n",
       "CCGCACACAGGTTAAA-1-s1d1                 1.0                 0.0   \n",
       "TCATTTGGTAATGGAA-1-s1d1                 0.0                 0.0   \n",
       "ACCACATAGGTGTCCA-1-s1d1                 1.0                 0.0   \n",
       "...                                     ...                 ...   \n",
       "TAGTAAGCAACTAGGG-8-s3d6                 0.0                 1.0   \n",
       "TGGTCCTTCGGCTAGC-8-s3d6                 1.0                 0.0   \n",
       "CGCTTGCGTTGTTGGA-8-s3d6                 0.0                 0.0   \n",
       "ACCCTCCCAGCCAGTT-8-s3d6                 0.0                 0.0   \n",
       "AGTGAACCATCCCGCT-8-s3d6                 0.0                 1.0   \n",
       "\n",
       "                         chr1-816868-817761  chr1-827067-827948  \\\n",
       "TAGTTGTCACCCTCAC-1-s1d1                 0.0                 0.0   \n",
       "CTATGGCCATAACGGG-1-s1d1                 0.0                 1.0   \n",
       "CCGCACACAGGTTAAA-1-s1d1                 0.0                 0.0   \n",
       "TCATTTGGTAATGGAA-1-s1d1                 0.0                 0.0   \n",
       "ACCACATAGGTGTCCA-1-s1d1                 0.0                 0.0   \n",
       "...                                     ...                 ...   \n",
       "TAGTAAGCAACTAGGG-8-s3d6                 0.0                 0.0   \n",
       "TGGTCCTTCGGCTAGC-8-s3d6                 0.0                 1.0   \n",
       "CGCTTGCGTTGTTGGA-8-s3d6                 0.0                 0.0   \n",
       "ACCCTCCCAGCCAGTT-8-s3d6                 0.0                 0.0   \n",
       "AGTGAACCATCCCGCT-8-s3d6                 0.0                 0.0   \n",
       "\n",
       "                         chr1-842497-843414  chr1-869472-870377  \\\n",
       "TAGTTGTCACCCTCAC-1-s1d1                 0.0                 0.0   \n",
       "CTATGGCCATAACGGG-1-s1d1                 0.0                 0.0   \n",
       "CCGCACACAGGTTAAA-1-s1d1                 0.0                 0.0   \n",
       "TCATTTGGTAATGGAA-1-s1d1                 0.0                 1.0   \n",
       "ACCACATAGGTGTCCA-1-s1d1                 0.0                 0.0   \n",
       "...                                     ...                 ...   \n",
       "TAGTAAGCAACTAGGG-8-s3d6                 0.0                 0.0   \n",
       "TGGTCCTTCGGCTAGC-8-s3d6                 0.0                 0.0   \n",
       "CGCTTGCGTTGTTGGA-8-s3d6                 0.0                 0.0   \n",
       "ACCCTCCCAGCCAGTT-8-s3d6                 0.0                 0.0   \n",
       "AGTGAACCATCCCGCT-8-s3d6                 0.0                 0.0   \n",
       "\n",
       "                         chr1-904343-905196  chr1-906441-907357  ...  \\\n",
       "TAGTTGTCACCCTCAC-1-s1d1                 0.0                 0.0  ...   \n",
       "CTATGGCCATAACGGG-1-s1d1                 0.0                 0.0  ...   \n",
       "CCGCACACAGGTTAAA-1-s1d1                 0.0                 0.0  ...   \n",
       "TCATTTGGTAATGGAA-1-s1d1                 0.0                 0.0  ...   \n",
       "ACCACATAGGTGTCCA-1-s1d1                 0.0                 0.0  ...   \n",
       "...                                     ...                 ...  ...   \n",
       "TAGTAAGCAACTAGGG-8-s3d6                 0.0                 0.0  ...   \n",
       "TGGTCCTTCGGCTAGC-8-s3d6                 1.0                 0.0  ...   \n",
       "CGCTTGCGTTGTTGGA-8-s3d6                 0.0                 1.0  ...   \n",
       "ACCCTCCCAGCCAGTT-8-s3d6                 0.0                 0.0  ...   \n",
       "AGTGAACCATCCCGCT-8-s3d6                 0.0                 0.0  ...   \n",
       "\n",
       "                         GL000205.2-88673-89483  GL000205.2-140307-141166  \\\n",
       "TAGTTGTCACCCTCAC-1-s1d1                     0.0                       0.0   \n",
       "CTATGGCCATAACGGG-1-s1d1                     0.0                       0.0   \n",
       "CCGCACACAGGTTAAA-1-s1d1                     0.0                       0.0   \n",
       "TCATTTGGTAATGGAA-1-s1d1                     0.0                       0.0   \n",
       "ACCACATAGGTGTCCA-1-s1d1                     0.0                       0.0   \n",
       "...                                         ...                       ...   \n",
       "TAGTAAGCAACTAGGG-8-s3d6                     1.0                       0.0   \n",
       "TGGTCCTTCGGCTAGC-8-s3d6                     0.0                       0.0   \n",
       "CGCTTGCGTTGTTGGA-8-s3d6                     0.0                       0.0   \n",
       "ACCCTCCCAGCCAGTT-8-s3d6                     0.0                       0.0   \n",
       "AGTGAACCATCCCGCT-8-s3d6                     0.0                       0.0   \n",
       "\n",
       "                         GL000195.1-30407-31261  GL000195.1-32211-33062  \\\n",
       "TAGTTGTCACCCTCAC-1-s1d1                     0.0                     0.0   \n",
       "CTATGGCCATAACGGG-1-s1d1                     0.0                     0.0   \n",
       "CCGCACACAGGTTAAA-1-s1d1                     1.0                     0.0   \n",
       "TCATTTGGTAATGGAA-1-s1d1                     1.0                     0.0   \n",
       "ACCACATAGGTGTCCA-1-s1d1                     0.0                     0.0   \n",
       "...                                         ...                     ...   \n",
       "TAGTAAGCAACTAGGG-8-s3d6                     0.0                     0.0   \n",
       "TGGTCCTTCGGCTAGC-8-s3d6                     0.0                     0.0   \n",
       "CGCTTGCGTTGTTGGA-8-s3d6                     0.0                     0.0   \n",
       "ACCCTCCCAGCCAGTT-8-s3d6                     0.0                     0.0   \n",
       "AGTGAACCATCCCGCT-8-s3d6                     0.0                     0.0   \n",
       "\n",
       "                         GL000219.1-39933-40839  GL000219.1-42172-43054  \\\n",
       "TAGTTGTCACCCTCAC-1-s1d1                     0.0                     0.0   \n",
       "CTATGGCCATAACGGG-1-s1d1                     0.0                     0.0   \n",
       "CCGCACACAGGTTAAA-1-s1d1                     0.0                     0.0   \n",
       "TCATTTGGTAATGGAA-1-s1d1                     0.0                     0.0   \n",
       "ACCACATAGGTGTCCA-1-s1d1                     0.0                     0.0   \n",
       "...                                         ...                     ...   \n",
       "TAGTAAGCAACTAGGG-8-s3d6                     0.0                     0.0   \n",
       "TGGTCCTTCGGCTAGC-8-s3d6                     1.0                     0.0   \n",
       "CGCTTGCGTTGTTGGA-8-s3d6                     1.0                     0.0   \n",
       "ACCCTCCCAGCCAGTT-8-s3d6                     0.0                     0.0   \n",
       "AGTGAACCATCCCGCT-8-s3d6                     0.0                     0.0   \n",
       "\n",
       "                         GL000219.1-44703-45584  GL000219.1-45726-46450  \\\n",
       "TAGTTGTCACCCTCAC-1-s1d1                     0.0                     0.0   \n",
       "CTATGGCCATAACGGG-1-s1d1                     0.0                     0.0   \n",
       "CCGCACACAGGTTAAA-1-s1d1                     0.0                     0.0   \n",
       "TCATTTGGTAATGGAA-1-s1d1                     0.0                     0.0   \n",
       "ACCACATAGGTGTCCA-1-s1d1                     0.0                     0.0   \n",
       "...                                         ...                     ...   \n",
       "TAGTAAGCAACTAGGG-8-s3d6                     0.0                     0.0   \n",
       "TGGTCCTTCGGCTAGC-8-s3d6                     0.0                     0.0   \n",
       "CGCTTGCGTTGTTGGA-8-s3d6                     1.0                     0.0   \n",
       "ACCCTCCCAGCCAGTT-8-s3d6                     0.0                     0.0   \n",
       "AGTGAACCATCCCGCT-8-s3d6                     0.0                     0.0   \n",
       "\n",
       "                         GL000219.1-99257-100160  KI270713.1-21434-22336  \n",
       "TAGTTGTCACCCTCAC-1-s1d1                      1.0                     0.0  \n",
       "CTATGGCCATAACGGG-1-s1d1                      0.0                     0.0  \n",
       "CCGCACACAGGTTAAA-1-s1d1                      0.0                     0.0  \n",
       "TCATTTGGTAATGGAA-1-s1d1                      0.0                     0.0  \n",
       "ACCACATAGGTGTCCA-1-s1d1                      0.0                     0.0  \n",
       "...                                          ...                     ...  \n",
       "TAGTAAGCAACTAGGG-8-s3d6                      0.0                     0.0  \n",
       "TGGTCCTTCGGCTAGC-8-s3d6                      0.0                     0.0  \n",
       "CGCTTGCGTTGTTGGA-8-s3d6                      1.0                     0.0  \n",
       "ACCCTCCCAGCCAGTT-8-s3d6                      0.0                     0.0  \n",
       "AGTGAACCATCCCGCT-8-s3d6                      0.0                     0.0  \n",
       "\n",
       "[22463 rows x 53020 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atac_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7ef9e",
   "metadata": {},
   "source": [
    "# import Transformer libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c04d72ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, T5ForConditionalGeneration\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc470467",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f2355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492c81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbde083",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ae27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50779d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efad96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bd87f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "433e7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9123f2f",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85c9d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8c65c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4393862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1076456",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "769ac56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40e69539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec90e215",
   "metadata": {},
   "source": [
    "# Position-wise Feed Forward-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cf90cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00031a40",
   "metadata": {},
   "source": [
    "# Embeddings and Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a9ba1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2fa6d",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8264be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e58b9",
   "metadata": {},
   "source": [
    "# Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dccf056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cdf71",
   "metadata": {},
   "source": [
    "# Reshape Data for T-5 Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86aac6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(22463, 12160)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "#convert anndata to numpy array\n",
    "gex_numpy_matrix = np.array(gex_)\n",
    "atac_numpy_matrix = np.array(atac_)\n",
    "print(type(atac_numpy_matrix))\n",
    "print(gex_numpy_matrix.shape)\n",
    "\n",
    "#binarize the gex data\n",
    "genex_ = sklearn.preprocessing.binarize(gex_numpy_matrix, threshold=1.0, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c33547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gex or atac data is present or 1 in binary obtain the name and replace binary value with name\n",
    "# print(gex_.shape)\n",
    "# print(genex_.shape)\n",
    "gex_[:] = genex_ # replace the gex data with the binary data\n",
    "\n",
    "gex_df_col = list(gex_df.columns.values) # get the column values\n",
    "\n",
    "# print(gex_dictionary)\n",
    "\n",
    "for key in gex_dictionary:\n",
    "    active_genes = ''\n",
    "    for cell in range(len(gex_.loc[key])):\n",
    "        if gex_.loc[key][cell] == 1:\n",
    "            active_genes += gex_df_col[cell] + ' '\n",
    "\n",
    "    gex_dictionary[key] = active_genes\n",
    "\n",
    "print(gex_dictionary)\n",
    "\n",
    "# print(len(gex_dictionary))\n",
    "#store the atac seq and gex of s1d1 for example in a .txt file \n",
    "#ex. {'TAGGTA': 'chr1-633515-634474 chr1-633525-634474 ', 'en': 'That is good.'}\n",
    "#call wrapper function \n",
    "#class TextLineTask(FunctionTask): found in text-to-text-transfer-transformer/t5/data/dataset_providers.py\n",
    "#call t5 small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208def6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "genex_\n",
    "#looks like the numpy array works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the sequence and split it into chunks\n",
    "def read_seq_split(split_dir): \n",
    "    split_dir = (split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"gex\", \"atac\"]: # for each label\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc61cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, T5ForConditionalGeneration, T5Tokenizer\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "#gex_df = adata_gex.to_df()\n",
    "#X = gex_df.drop(['target'],axis=1).values   # independant features\n",
    "#y = gex_df['target'].values                 # dependant variable\n",
    "#train_texts, val_texts, train_labels, val_labels = train_test_split(x, y, train_texts, train_labels, test_size=.2)\n",
    "#train_text, train_labels = read_seq_split(gex_df)\n",
    "#train_text =  \n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "train_encodings = tokenizer(adata_gex, truncation=True, padding=True)\n",
    "#val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "#test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e29c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_texts, train_labels = read_seq_split(adata_gex) \n",
    "#test_texts, test_labels = read_seq_split('figures/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ed70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_atac.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ccca32",
   "metadata": {},
   "source": [
    "We are now going to print out the number of observations and features of our GEX and ATAC-seq data. \n",
    "\n",
    "Few things to note before we proceed:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The GEX data has {adata_gex.n_obs} observations and {adata_gex.n_vars} features.\")\n",
    "print(f\"The ATAC data has {adata_atac.n_obs} observations and {adata_atac.n_vars} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75164b1d",
   "metadata": {},
   "source": [
    "# TRANSFORMER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710f6313",
   "metadata": {},
   "source": [
    "The Transformer T-5 small model will take in a custom dataset. This model relies solely on training therefore it is important that we have the proper pretraining before turning in this method for single cell sequencing analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the sequence and split it into chunks\n",
    "def read_seq_split(split_dir): \n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"\", \"\"]: # for each label\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf4e4d",
   "metadata": {},
   "source": [
    "Loading up the data so the sequences can be read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac24423",
   "metadata": {},
   "source": [
    "Train our transformer using this the train_test_split() function. This wraps input validation and application to input data into a single call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wraps input validation and application to input data into a single call\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca87ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIADataset(torch.utils.data.Dataset): # create a custom dataset for neurips mia model\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a66da",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff8d03",
   "metadata": {},
   "source": [
    "Using Pytorch trainer, we want to train our model. t-5 small and transformers all around rely solely on this training data so this is definatley the most important aspect of the code. Doing so will play a massive role in how we analyze this single cell data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec51714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  assuming we want to use trainer in leiu of custom pytorch trainer\n",
    "# need to change training args based on raz input on the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"t5-small\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11d03f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e509739a",
   "metadata": {},
   "source": [
    "# Baseline Models given by NeurIPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63729a4",
   "metadata": {},
   "source": [
    "a few statistical metrics given to us in the NeurIPS competition that shows how are algorithm performs. <br>\n",
    "The tests include:<br>\n",
    "**rmse** - **r**oot **m**ean **s**quare **e**rror<br>\n",
    "**baseline_linear** - linear regressor test<br>\n",
    "**baseline_mean** - mean test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93eac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(true_test_mod2, pred_test_mod2):\n",
    "    if pred_test_mod2.var[\"feature_types\"][0] == \"GEX\":\n",
    "        return  mean_squared_error(true_test_mod2.layers[\"log_norm\"].toarray(), pred_test_mod2.X, squared=False)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only set up to calculate RMSE for GEX data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2834dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_linear(input_train_mod1, input_train_mod2, input_test_mod1):\n",
    "    '''Baseline method training a linear regressor on the input data'''\n",
    "    input_mod1 = ad.concat(\n",
    "        {\"train\": input_train_mod1, \"test\": input_test_mod1},\n",
    "        axis=0,\n",
    "        join=\"outer\",\n",
    "        label=\"group\",\n",
    "        fill_value=0,\n",
    "        index_unique=\"-\", \n",
    "    )\n",
    "    \n",
    "    # Binarize ATAC \n",
    "    if input_train_mod1.var[\"feature_types\"][0] == \"ATAC\":\n",
    "        input_mod1.X[input_mod1.X > 1] = 1\n",
    "    elif input_train_mod2.var[\"feature_types\"][0] == \"ATAC\":\n",
    "        input_train_mod2.X[input_mod1.X > 1] = 1\n",
    "    \n",
    "    # Do PCA on the input data\n",
    "    logging.info('Performing dimensionality reduction on modality 1 values...')\n",
    "    embedder_mod1 = TruncatedSVD(n_components=50)\n",
    "    mod1_pca = embedder_mod1.fit_transform(input_mod1.X)\n",
    "    \n",
    "    logging.info('Performing dimensionality reduction on modality 2 values...')\n",
    "    embedder_mod2 = TruncatedSVD(n_components=50)\n",
    "    mod2_pca = embedder_mod2.fit_transform(input_train_mod2.layers[\"log_norm\"])\n",
    "    \n",
    "    # split dimred mod 1 back up for training\n",
    "    X_train = mod1_pca[input_mod1.obs['group'] == 'train']\n",
    "    X_test = mod1_pca[input_mod1.obs['group'] == 'test']\n",
    "    y_train = mod2_pca\n",
    "    \n",
    "    assert len(X_train) + len(X_test) == len(mod1_pca)\n",
    "    \n",
    "    logging.info('Running Linear regression...')\n",
    "    \n",
    "    reg = LinearRegression()\n",
    "    \n",
    "    # Train the model on the PCA reduced modality 1 and 2 data\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    \n",
    "    # Project the predictions back to the modality 2 feature space\n",
    "    y_pred = y_pred @ embedder_mod2.components_\n",
    "    \n",
    "    pred_test_mod2 = ad.AnnData(\n",
    "        X = y_pred,\n",
    "        obs = input_test_mod1.obs,\n",
    "        var = input_train_mod2.var,\n",
    "    \n",
    "    )\n",
    "    \n",
    "    # Add the name of the method to the result\n",
    "    pred_test_mod2.uns[\"method\"] = \"linear\"\n",
    "    \n",
    "    return pred_test_mod2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32924350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_mean(input_train_mod1, input_train_mod2, input_test_mod1):\n",
    "    '''Dummy method that predicts mean(input_train_mod2) for all cells'''\n",
    "    logging.info('Calculate mean of the training data modality 2...')\n",
    "    y_pred = np.repeat(input_train_mod2.layers[\"log_norm\"].mean(axis=0).reshape(-1,1).T, input_test_mod1.shape[0], axis=0)\n",
    "    \n",
    "    # Prepare the ouput data object\n",
    "    pred_test_mod2 = ad.AnnData(\n",
    "        X=y_pred,\n",
    "        obs=input_test_mod1.obs,\n",
    "        var=input_train_mod2.var,\n",
    "    )\n",
    "    \n",
    "    pred_test_mod2.uns[\"method\"] = \"mean\"\n",
    "\n",
    "    return pred_test_mod2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41aa69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d61e67d4406f83661a218a7594034be74564666d0640d3900a3e99845865d0f0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
